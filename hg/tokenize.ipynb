{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Only those who will risk going too far can possibly find out how far one can go.\"\n",
    "tokenized_str = string.split()\n",
    "print(tokenized_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_word2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))}\n",
    "print(token_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [token_word2idx[token] for token in tokenized_str]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT\n",
    "import re, collections\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "# 這裡的 vocab 裡的字都要用空格分開\n",
    "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n",
    "'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "num_merges = 15\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    # print(dict(pairs))\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    print(best)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    # print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "string = \"Only those who will risk going too far can possibly find out how far one can go.\"\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\" #直接叫model名字\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoded_str = tokenizer(string, padding=True, truncation=True) \n",
    "print(encoded_str)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_str.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertTokenizer\n",
    "\n",
    "# distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "# encoded_str = distilbert_tokenizer(string, padding=True, truncation=True) \n",
    "# print(encoded_str)\n",
    "# tokens = tokenizer.convert_ids_to_tokens(encoded_str.input_ids)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_array = [\n",
    "    string,\n",
    "    \"Baby shark, doo doo doo doo doo doo, Baby shark!\"\n",
    "]\n",
    "\n",
    "encoded_str_arr = tokenizer(string_array, padding=True, truncation=True)\n",
    "encoded_str_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "sentiment = load_dataset(\"poem_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"verse_text\"], padding=True, truncation=True)\n",
    "print(tokenize(sentiment[\"train\"][:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)\n",
    "print(sentiment_encoded[\"train\"].column_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle-misc-GnC41mwT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
